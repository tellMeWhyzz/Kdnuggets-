我们要爬取文本，图片，标题，超链接并存储为json格式
===

1.先从文本开始
---
今天首先分析了

# 爬取单章节的文字内容
from urllib import request<br>
from bs4 import BeautifulSoup<br>

if __name__ == '__main__':<br>
    # 第6章的网址<br>
    url = 'http://www.136book.com/mieyuechuanheji/ewqlwb/'<br>
    head = {}<br>
    # 使用代理<br>
    head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'<br>
    req = request.Request(url, headers = head)<br>
    response = request.urlopen(req)<br>
    html = response.read()<br>
    # 创建request对象<br>
    soup = BeautifulSoup(html, 'lxml')<br>
    # 找出div中的内容<br>
    soup_text = soup.find('div', id = 'content')<br>
    # 输出其中的文本<br>
    print(soup_text.text)<br>

改写到我的代码变成这个
===


from urllib import request<br>
from bs4 import BeautifulSoup<br>

if __name__ == '__main__':<br>
    # 第6章的网址<br>
    url = 'https://www.kdnuggets.com/2009/12/f-ibm-fordham-curriculum-business-analytics.html'<br>
    head = {}<br>
    # 使用代理<br>
    head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'<br>
    req = request.Request(url, headers = head)<br>
    response = request.urlopen(req)<br>
    html = response.read()<br>
    # 创建request对象<br>
    soup = BeautifulSoup(html, 'lxml')<br>
    # 找出div中的内容<br>
    soup_text = soup.find('div', id = 'post-')<br>
    # 输出其中的文本<br>
    print(soup_text.text)<br>
    
    
仿照这个代码
===
# 爬取目录页面-章节对应的网址<br>
from urllib import request<br>
from bs4 import BeautifulSoup<br>

if __name__ == '__main__':<br>
    # 目录页<br>
    url = 'http://www.136book.com/mieyuechuanheji/'<br>
    head = {}<br>
    head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'<br>
    req = request.Request(url, headers = head)<br>
    response = request.urlopen(req)<br>
    html = response.read()<br>
    # 解析目录页<br>
    soup = BeautifulSoup(html, 'lxml')<br>
    # find_next找到第二个div<br>
    soup_texts = soup.find('div', id = 'book_detail', class_= 'box1').find_next('div')<br>
    # 遍历ol的子节点，打印出章节标题和对应的链接地址<br>
    for link in soup_texts.ol.children:<br>
        if link != '\n':<br>
            print(link.text + ':  ', link.a.get('href'))<br>
            
 我的代码是这个
 ===
from urllib import request<br>
from bs4 import BeautifulSoup<br>

if __name__ == '__main__':<br>
    # 目录页<br>
    url = 'https://www.kdnuggets.com/2010/11/index.html'<br>
    head = {}<br>
    head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'<br>
    req = request.Request(url, headers = head)<br>
    response = request.urlopen(req)<br>
    html = response.read()<br>
    # 解析目录页<br>
    soup = BeautifulSoup(html, 'lxml')<br>
 #    find_next找到第二个<div><br>
#     soup_texts = soup.find('div', id = 'post-')<br>
#     print(soup_texts)<br>
   
    # 遍历ul的子标签，这里不知道为什么要加一个'\n', 如果不加就不行<br>
    # 这里用find_all方法找到所有的li标签也可以，用find方法也可以<br>
    soup_texts = soup.find_all('li')<br>
    for link in soup_texts:<br>
        if link != '\n':<br>
            print(link.text + ':  ', link.a.get('href'))<br>
#               print(link.text)<br>
        print()<br>
        
将爬取的东西存储下来，我这里存储的是json格式，模仿的是txt格式
===
#!/usr/bin/env python  <br>
#-*-coding:utf-8-*-  <br>
      
import requests, json<br>
import sys<br>
from bs4 import BeautifulSoup<br>
 
#处理编码问题<br>
reload(sys)  <br>
sys.setdefaultencoding("utf-8")  <br>
 
user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) Gecko/20100101 Firefox/55.0' #用户代理<br>
headers = {'User-Agent': user_agent}<br>
cookies = {'__cfduid':'d56050ac3d7e8ae5a71a6c49829aefb281504577002', 'ASPSESSIONIDSQCSSQRA':'GDFBBGMCACLGNJMNJFODPPLL',<br>
 'Hm_lpvt_0154745fe13860e5efacd87a86a7cdea':'1504587470', 'bdshare_firstime':'1504577006948',<br>
 'cf_clearance':'2faf34a20a29d30d97d3fe3cc4594fdb207978da-1504577006-86400'} #网站在电脑上留下的Cookies<br>
res = requests.get('http://seputu.com/', headers = headers, cookies = cookies)<br>
 
soup = BeautifulSoup(res.text)<br>
content = []<br>
for mulu in soup.find_all(class_ = 'mulu'):<br>
	h2 = mulu.find('h2')<br>
	if h2:<br>
		title = h2.string<br>
		chapter = []<br>
		for a in mulu.find_all('a'):<br>
			href = a.get('href')<br>
			name = a.get('title')<br>
			chapter.append({'href': href, 'chapter': name})<br>
		content.append({'book': title, 'content': chapter})<br>
with open('daomubiji.json', 'wb') as fp:  #将所得的数据存储为json文件<br>
	json.dump(content, fp = fp, ensure_ascii = False, indent = 4, sort_keys = True)<br>
    #dump函数有很多参数，这里也用到了好几个，第一个是目标object，第二个是要写入的文件对象，第三个处理非ascii码的字符<br>
    #第四个是缩进，格式化显示，最后一个是排序<br>
    
    我的代码是
    ===
from urllib import request<br>
from bs4 import BeautifulSoup<br>
import json<br>
from imp import reload<br>
import sys<br>

#处理编码问题<br>
reload(sys)  <br>

if __name__ == '__main__':<br>
    # 目录页<br>
    url = 'https://www.kdnuggets.com/2010/11/index.html'<br>
    head = {}<br>
    head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'<br>
    req = request.Request(url, headers = head)<br>
    response = request.urlopen(req)<br>
    html = response.read()<br>
    # 解析目录页<br>
    soup = BeautifulSoup(html, 'lxml')<br>
    # find_next找到第二个<div><br>
   
    # 遍历ul的子标签，这里不知道为什么要加一个'\n', 如果不加就不行<br>
    # 这里用find_all方法找到所有的li标签也可以<br>
    soup_texts = soup.find_all('li')<br>
    content = []<br>
    for link in soup_texts:<br>
        chapter = []<br>
        if link != '\n':<br>
#             print(link.text + ':  ', link.a.get('href'))<br>
            name = link.text<br>
            href = link.a.get('href')<br>
            chapter.append({'title' : name, 'href' : href})<br>
            content.append({'content' : chapter})<br>
#             print()<br>
#     for i in content:<br>
#         print(i)<br>
    # 将所得的数据存储为json文件<br>
    # 注意，在这里我之前一直报TypeError: a bytes-like object is required, not 'str'这个错误<br>
    # 解决方法: 讲这个wb改写为w即可，这里告诉你需要的是一个字节对象，而不是str对象<br>
    # 参考链接:https://www.cnblogs.com/tmblog/p/9357637.html<br>
    with open('kdnuggets.json', 'w') as fp:  <br>
        #dump函数有很多参数，这里也用到了好几个，第一个是目标object，第二个是要写入的文件对象，第三个处理非ascii码的字符<br>
        #第四个是缩进，格式化显示，最后一个是排序<br>
        json.dump(content, fp = fp, ensure_ascii = False, indent = 4, sort_keys = True)<br>
